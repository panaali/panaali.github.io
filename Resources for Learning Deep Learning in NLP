---
published: false
layout: post
date: '2019-06-27 13:06:00 -0500'
categories: deep-learning
---

These are resources that I found useful.

## Tutorials

*   [Stanford CS231n Lecture 10](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture10.pdf) [for LSTM and Seq2Seq]
*   [Stanford CS224d](https://cs224d.stanford.edu/syllabus.html) [Basic of DL in NLP]
*   [Deep Learning book Chapter 10](http://www.deeplearningbook.org/contents/rnn.html) [for LSTM and Seq2Seq]
*   [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
*   [NLP course in USF](https://github.com/fastai/course-nlp/blob/master/8-translation-transformer.ipynb) [with Transformer]
*   [DL NLP History](http://a%20review%20of%20the%20neural%20history%20of%20natural%20language%20processing/)
*   Dissecting BERT [1](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3), [2](https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73), [3](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f)
*   Dissecting Transformer-XL [1](https://medium.com/@mromerocalvo/dissecting-transformer-xl-90963e274bd7), [2](https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-9-transformer-xl-attentive-language-models-beyond-a-fixed-length-context-d045abf6db8)
*   On Sparse Transformer [Blog](https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-12-generating-long-sequences-with-sparse-transformers-f290eadfb95b)
*   On GPT2 [Blog](https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-1-language-models-are-unsupervised-multitask-learners-fdb7016d8aad)
*   On Elmo, Bert, and Transformer [Blog](http://jalammar.github.io/illustrated-bert/)


## Most Important Papers

*   1986 - RNN
*   1997 - [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)
*   2013 - [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
*   2014 - [Encoder-Decoder LSTM](https://arxiv.org/pdf/1409.3215.pdf) [=Seq2Seq]
*   2016 - [Attention](https://distill.pub/2016/augmented-rnns/)
*   Mar 2018 - [Elmo](https://arxiv.org/pdf/1802.05365.pdf)
*   May 2018 - [UlmFit](https://arxiv.org/pdf/1801.06146.pdf), [Blog](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html), [2](http://ruder.io/nlp-imagenet/) [Transfer Learning for NLP]
*   Jun 2018 - [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [Blog](https://openai.com/blog/language-unsupervised/)
*   Jun 2018 - [Transformer](https://arxiv.org/pdf/1706.03762.pdf) [Fixed length, non-recurrent]
*   Oct 2018 - [BERT](https://arxiv.org/pdf/1810.04805.pdf) [Bidirectional Transformer]
*   Feb 2019 - [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [Blog](https://openai.com/blog/better-language-models)
*   Jan 2019 - [Transformer-XL](https://arxiv.org/pdf/1901.02860.pdf) [Dynamic Length]
*   Apr 2019 - [Sparse Transformers](https://arxiv.org/pdf/1904.10509.pdf) [Generative model]
